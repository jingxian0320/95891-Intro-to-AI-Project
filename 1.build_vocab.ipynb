{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"1.build_vocab.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"id":"DBX05_cHNbBG","colab_type":"code","outputId":"76d72479-aeb9-4264-e010-4d25fce43668","executionInfo":{"status":"ok","timestamp":1575152785508,"user_tz":300,"elapsed":21958,"user":{"displayName":"Jingxian Bao","photoUrl":"","userId":"12028084690577949788"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"arlYhja9gyF7","colab_type":"code","outputId":"8566e12d-23b4-4d8c-da68-20d7c5d62090","executionInfo":{"status":"ok","timestamp":1575152794257,"user_tz":300,"elapsed":30698,"user":{"displayName":"Jingxian Bao","photoUrl":"","userId":"12028084690577949788"}},"colab":{"base_uri":"https://localhost:8080/","height":343}},"source":["!pip install multi_key_dict\n","!pip install word2number"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting multi_key_dict\n","  Downloading https://files.pythonhosted.org/packages/6d/97/2e9c47ca1bbde6f09cb18feb887d5102e8eacd82fbc397c77b221f27a2ab/multi_key_dict-2.0.3.tar.gz\n","Building wheels for collected packages: multi-key-dict\n","  Building wheel for multi-key-dict (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for multi-key-dict: filename=multi_key_dict-2.0.3-cp36-none-any.whl size=9300 sha256=45e83379a6715539349fdf86cf0c0319a09e1d1ef69b73fc2889df30669d6458\n","  Stored in directory: /root/.cache/pip/wheels/1d/26/5b/ac32658fddc88f045a516406ff69cafabaab2602eb96bbe2f0\n","Successfully built multi-key-dict\n","Installing collected packages: multi-key-dict\n","Successfully installed multi-key-dict-2.0.3\n","Collecting word2number\n","  Downloading https://files.pythonhosted.org/packages/4a/29/a31940c848521f0725f0df6b25dca8917f13a2025b0e8fcbe5d0457e45e6/word2number-1.1.zip\n","Building wheels for collected packages: word2number\n","  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for word2number: filename=word2number-1.1-cp36-none-any.whl size=5588 sha256=1a22b0fea34b8137735bc9d794260f72ef696c90d7be5c1cd43db1bd5e8d329a\n","  Stored in directory: /root/.cache/pip/wheels/46/2f/53/5f5c1d275492f2fce1cdab9a9bb12d49286dead829a4078e0e\n","Successfully built word2number\n","Installing collected packages: word2number\n","Successfully installed word2number-1.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BcALHtaDQOa2","colab_type":"code","outputId":"1749a1fa-7974-4a16-c14e-98464eb62c8f","executionInfo":{"status":"ok","timestamp":1575152796549,"user_tz":300,"elapsed":32984,"user":{"displayName":"Jingxian Bao","photoUrl":"","userId":"12028084690577949788"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["import nltk\n","from nltk.stem import WordNetLemmatizer\n","import pickle\n","import argparse\n","from collections import Counter\n","import json\n","import os\n","from tqdm import *\n","import numpy as np\n","import re\n","import random\n","# allows multiple keys for one key-value pair\n","from multi_key_dict import multi_key_dict\n","from word2number import w2n\n","nltk.download('wordnet')\n","nltk.download('punkt')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"DgBLu0jyaJf_","colab_type":"code","colab":{}},"source":["DATA_PATH = \"/content/drive/My Drive/Intro to AI Project/data/\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"28T_kf6suwyZ","colab_type":"text"},"source":["##Vocabulary Wrapper"]},{"cell_type":"code","metadata":{"id":"VVsPsIo1QdLd","colab_type":"code","colab":{}},"source":["class Vocabulary(object):\n","    \"\"\"Simple vocabulary wrapper.\"\"\"\n","    def __init__(self):\n","        self.word2idx = {}\n","        self.idx2word = {}\n","        self.idx = 0\n","\n","    def add_word(self, word, idx=None):\n","        if idx is None:\n","            if not word in self.word2idx:\n","                self.word2idx[word] = self.idx\n","                self.idx2word[self.idx] = word\n","                self.idx += 1\n","            return self.idx\n","        else:\n","            if not word in self.word2idx:\n","                self.word2idx[word] = idx\n","                if idx in self.idx2word.keys():\n","                    self.idx2word[idx].append(word)\n","                else:\n","                    self.idx2word[idx] = [word]\n","\n","                return idx\n","\n","    def __call__(self, word):\n","        if not word in self.word2idx:\n","            return self.word2idx['<pad>']\n","        return self.word2idx[word]\n","\n","    def __len__(self):\n","        return len(self.idx2word)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"grn4K5mcu3F6","colab_type":"text"},"source":["##Rules for ingredients cleaning"]},{"cell_type":"code","metadata":{"id":"mbrnM98SRK-A","colab_type":"code","outputId":"e78a704a-580a-42ec-8e57-1622aa1f929e","executionInfo":{"status":"ok","timestamp":1575152798149,"user_tz":300,"elapsed":34570,"user":{"displayName":"Jingxian Bao","photoUrl":"","userId":"12028084690577949788"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["\n","lemmatizer = WordNetLemmatizer()\n","#\\d: decimal; \\s: whitespace; \n","# pattern of units\n","num_units = re.compile(r'(\\d)+(\\s)?(cubes?|cups?|c(a?ns?)?|[tT]|bulbs?|sprigs?|glass(es)?|dice|blocks?|an?|l|fl(uid)?\\.?|ears?|lea(f|ves)|jars?|cartons?|strips?|heads?|wedges?|envelopes?|pints?|stalks?|sticks?|pinch(es)?|qts?|quarts?|handful|weight|bottles?|grinds?|tb\\.?|lbs?\\.?|oz\\.?|mls?|g|cloves?|containers?|tablespoons?|teaspoons?|dash(es)?|pounds?|pinch|box(es)?|cans?|(milli)?lit[er]{2}s?|pkg\\.?|pack(et)s?|packages?|whole|bars?|bags?|tbsps?\\.?|tbs\\.?|ts|tsps?\\.?|ounces?|dash|pieces?|slices?|bunch(es)?|sticks?|fl\\.?|gallons?|squares?|knobs?|grams?|kgs?|tub(es)?|kilograms?|tins?|%|drizzles?|splash(es)?|chunks?|inch(es)?)(\\s|$)')\n","   \n","replace_dict_ingrs = multi_key_dict()\n","replace_dict_ingrs['&',\"'n\"] = ' and '\n","replace_dict_ingrs['-'] = ' '\n","\n","# preprocess text for ingredients \n","def get_ingredient(det_ingr):\n","    det_ingr = det_ingr['text'].lower()\n","    re.sub(\"[\\(\\[\\{].*?[\\)\\]\\}]\", \"\", det_ingr) #remove text in parentheses\n","    re.sub(num_units,\"\",det_ingr) #remove units\n","    for k_list in replace_dict_ingrs.keys():\n","      for k in k_list:\n","        if k in det_ingr:\n","          det_ingr = det_ingr.replace(k,replace_dict_ingrs[k]) #replace characters like (& n -)        \n","    cleaned_list = []\n","    for word in det_ingr.split():\n","      if word.isalpha():  # word only, no numbers or other symbols\n","        cleaned_list.append(lemmatizer.lemmatize(word)) \n","    det_ingr_cleaned = \" \".join(cleaned_list)\n","    det_ingr_cleaned = det_ingr_cleaned.replace(' ', '_')\n","    return det_ingr_cleaned\n","\n","get_ingredient({\"text\":\"1g salt salmon 100g tomato\"})\n"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'salt_salmon_tomato'"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"8IpqSv8YZoru","colab_type":"text"},"source":["##Rules for instructions cleaning"]},{"cell_type":"code","metadata":{"id":"ai_dE57KxL6O","colab_type":"code","outputId":"02e15fb8-dd82-4629-e795-29d89234a940","executionInfo":{"status":"ok","timestamp":1575152798151,"user_tz":300,"elapsed":34566,"user":{"displayName":"Jingxian Bao","photoUrl":"","userId":"12028084690577949788"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# Interesting, using multi_key_dict to reduce dimensionality/standardize\n","\n","replace_dict_instrs = multi_key_dict()\n","replace_dict_instrs[' nonfat ', ' non-fat '] = ' fat-free '\n","replace_dict_instrs['low-fat', 'reduced-fat'] = ' fat-reduced '\n","replace_dict_instrs[' flatleaf '] = ' flat-leaf '\n","replace_dict_instrs['&',\"'n\"] = ' and '\n","replace_dict_instrs['!','?'] = '.'\n","replace_dict_instrs[';',':'] = '.'\n","replace_dict_instrs['-','(',')',\"[\",']',\"#\",\"$\"] = ' '\n","replace_dict_instrs[' a ',\" an \"] = ' 1 '\n","\n","# unit conversions: standardize units\n","unit_conversions = multi_key_dict()\n","unit_conversions['pounds', 'pound', 'lbs', 'lb'] = ('g', 453.6)\n","unit_conversions['ounces', 'ounce', 'ozs', 'oz', 'weight'] = ('g', 28.35)\n","unit_conversions['can', 'cans', 'cn'] = ('can', 1)\n","unit_conversions['pints', 'pint', 'pts', 'pt'] = ('l', 0.4732)\n","unit_conversions['quarts', 'quart', 'qts', 'qt'] = ('l', 1.946352946)\n","unit_conversions['cups', 'cup', 'c'] = ('l', 0.2366)\n","unit_conversions['cubes', 'cube'] = ('cube', 1)\n","unit_conversions['fluid', 'fl'] = ('l', 0.02957)\n","unit_conversions['tablespoons', 'tablespoon', 'tbsps', 'tbsp', 'tb', 'tbs', 'T'] = ('l', 0.01479)\n","unit_conversions['teaspoons', 'teaspoon', 'tsps', 'tsp', 't', 'ts'] = ('l', 0.004929)\n","unit_conversions['milliliters', 'millilitres', 'ml'] = ('l', 0.001)\n","unit_conversions['gram', 'gs', 'grams'] = ('g', 1)\n","unit_conversions['kilogram', 'kgs', 'kg', 'kilograms'] = ('g', 0.001)\n","\n","def get_instruction(instruction, instruction_mode=True):\n","    instruction = instruction.lower()\n","    re.sub(\"[\\(\\[\\{].*?[\\)\\]\\}]\", \"\", instruction) #remove text in parentheses\n","    for k_list in replace_dict_instrs.keys():\n","      for k in k_list:\n","        if k in instruction:\n","          instruction = instruction.replace(k,replace_dict_instrs[k]) # use rules to standardize labels\n","    instruction = instruction.replace('half','1/2') # should it be inluded above?\n","    cleaned_list = []\n","    # unit conversion\n","    words = instruction.split()\n","    for i in range (len(words)):\n","      if words[i] in unit_conversions:\n","        try:\n","          # the word before unit is most likely a number\n","          num = w2n.word_to_num(words[i-1])\n","          # unit conversion\n","          num = int(num/unit_conversions[words[i]][1])\n","          cleaned_list[-1] = str(num)\n","          cleaned_list.append(unit_conversions[words[i]][0])\n","          continue\n","        except:\n","          pass\n","        try:\n","          num = words[i-1]\n","          if '/' in num:\n","            num = float(num.split('/')[0])/float(num.split('/')[1])\n","          else:\n","            num = float(num)\n","          num = num/unit_conversions[words[i]][1]\n","          cleaned_list[-1] = str(num)\n","          cleaned_list.append(unit_conversions[words[i]][0])\n","          continue\n","        except:\n","          pass\n","      else:\n","        cleaned_list.append(words[i])\n","                                      \n","    instruction = \" \".join(cleaned_list)\n","    # instruction too short to be valid\n","    if len(instruction) < 3:  \n","      return \"\"\n","    # Remove \"1.\" or \"2.\" for steps -> for model  \n","    if len(instruction) > 0 and instruction[0].isdigit() and instruction[1] == '.' and instruction_mode:\n","        instruction = instruction[2:].strip()\n","    return instruction\n"," \n","print (get_instruction(\"1.Arrange 3/5 kg veggies on top of two kg salmon 'n seal foil tightly to create a tightly sealed pocket.\"))\n","print (get_instruction(\"1.\"))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["arrange 600.0 g veggies on top of 2000 g salmon and seal foil tightly to create 1 tightly sealed pocket.\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Kaaih9DuqL5T","colab_type":"text"},"source":["# Cluster similar ingredients"]},{"cell_type":"code","metadata":{"id":"XGG3YEcuUEOz","colab_type":"code","colab":{}},"source":["def cluster_ingredients(counter_ingrs):\n","    mydict = dict()\n","    mydict_ingrs = dict()\n","\n","    for k, v in counter_ingrs.items():  #counter_ingrs is a dict\n","\n","        w1 = k.split('_')[-1] #last word\n","        w2 = k.split('_')[0]  #first word\n","        lw = [w1, w2]\n","        if len(k.split('_')) > 1:\n","            w3 = k.split('_')[0] + '_' + k.split('_')[1] #first two words\n","            w4 = k.split('_')[-2] + '_' + k.split('_')[-1] #last two words\n","\n","            lw = [w1, w2, w4, w3]\n","\n","        gotit = 0\n","        for w in lw:\n","            if w in counter_ingrs.keys():\n","                # check if its parts are\n","                parts = w.split('_')\n","                if len(parts) == 2:\n","                    if parts[0] in counter_ingrs.keys():\n","                        w = parts[0]\n","                    elif parts[1] in counter_ingrs.keys():\n","                        w = parts[1]\n","                if w in mydict.keys():\n","                    mydict[w] += v\n","                    mydict_ingrs[w].append(k)\n","                else:\n","                    mydict[w] = v\n","                    mydict_ingrs[w] = [k]\n","                gotit = 1\n","                break\n","        if gotit == 0:\n","            mydict[k] = v\n","            mydict_ingrs[k] = [k]\n","\n","    return mydict, mydict_ingrs\n","\n","\n","def update_counter(list_, counter_toks, istrain=False):\n","    for sentence in list_:\n","        tokens = nltk.tokenize.word_tokenize(sentence)\n","        if istrain:\n","            counter_toks.update(tokens)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fSY1Zt-zYcEP","colab_type":"code","colab":{}},"source":["def build_vocab_recipe1m():\n","    print (\"Loading data...\")\n","    dets = json.load(open(os.path.join(DATA_PATH, 'det_ingrs.json'), 'r'))\n","    layer1 = json.load(open(os.path.join(DATA_PATH, 'layer1.json'), 'r'))\n","\n","    print(\"Loaded data.\")\n","    print(\"Found %d recipes in the dataset.\" % (len(layer1)))\n","    \n","    # dict with id as key and number as value\n","    idx2ind = {}\n","    for i, entry in enumerate(dets):\n","        idx2ind[entry['id']] = i\n","\n","    # creating new file paths\n","    ingrs_file = DATA_PATH + 'allingrs_count.pkl'\n","    instrs_file = DATA_PATH + 'allwords_count.pkl'\n","    \n","\n","    #####\n","    # 1. Count words in dataset and clean\n","    #####\n","    \n","    # if file exist then load files\n","    if os.path.exists(ingrs_file) and os.path.exists(instrs_file):\n","        print (\"loading pre-extracted word counters\")\n","        counter_ingrs = pickle.load(open(DATA_PATH + 'allingrs_count.pkl', 'rb'))\n","        counter_toks = pickle.load(open(DATA_PATH + 'allwords_count.pkl', 'rb'))\n","    else:\n","        counter_toks = Counter()\n","        counter_ingrs = Counter()\n","\n","        for i, entry in tqdm(enumerate(layer1)):\n","\n","            # get all instructions for this recipe\n","            instrs = entry['instructions']\n","\n","            instrs_list = []\n","            ingrs_list = []\n","\n","            # retrieve pre-detected ingredients for this entry\n","            det_ingrs = dets[idx2ind[entry['id']]]['ingredients']\n","            #print (det_ingrs)\n","            valid = dets[idx2ind[entry['id']]]['valid']\n","\n","            for j, det_ingr in enumerate(det_ingrs):\n","                if len(det_ingr) > 0 and valid[j]:\n","                    #print (det_ingr)\n","                    det_ingr_undrs = get_ingredient(det_ingr)\n","                    #print (det_ingr_undrs)\n","                    if len(det_ingr_undrs):\n","                      ingrs_list.append(det_ingr_undrs)\n","\n","            # get raw text for instructions of this entry\n","            acc_len = 0\n","            for instr in instrs:\n","                instr = instr['text']\n","                instr = get_instruction(instr)\n","                if len(instr) > 0:\n","                    instrs_list.append(instr)\n","                    acc_len += len(instr)\n","\n","            # discard recipes with too few or too many ingredients or instruction words\n","            if len(ingrs_list) < 2 or len(instrs_list) < 2 \\\n","                    or len(instrs_list) >= 20 or len(ingrs_list) >= 20 \\\n","                    or acc_len < 20:\n","                continue\n","\n","            # tokenize sentences and update counter\n","            update_counter(instrs_list, counter_toks, istrain=entry['partition'] == 'train')\n","            title = nltk.tokenize.word_tokenize(entry['title'].lower())\n","            if entry['partition'] == 'train':\n","                counter_toks.update(title)\n","            if entry['partition'] == 'train':\n","                counter_ingrs.update(ingrs_list)\n","        pickle.dump(counter_ingrs, open(ingrs_file, 'wb'))\n","        pickle.dump(counter_toks, open(instrs_file, 'wb'))\n","\n","\n","    counter_ingrs, cluster_ingrs = cluster_ingredients(counter_ingrs)\n","\n","    # If the word frequency is less than 'threshold', then the word is discarded.\n","    words = [word for word, cnt in counter_toks.items() if cnt >= 10]\n","    ingrs = {word: cnt for word, cnt in counter_ingrs.items() if cnt >= 3}\n","\n","    # Recipe vocab\n","    # Create a vocab wrapper and add some special tokens.\n","    vocab_toks = Vocabulary()\n","    vocab_toks.add_word('<start>')\n","    vocab_toks.add_word('<end>')\n","    vocab_toks.add_word('<eoi>')\n","\n","    # Add the words to the vocabulary.\n","    for i, word in enumerate(words):\n","        vocab_toks.add_word(word)\n","    vocab_toks.add_word('<pad>')\n","\n","    # Ingredient vocab\n","    # Create a vocab wrapper for ingredients\n","    vocab_ingrs = Vocabulary()\n","    idx = vocab_ingrs.add_word('<end>')\n","    # this returns the next idx to add words to\n","    # Add the ingredients to the vocabulary.\n","    for k, _ in ingrs.items():\n","        for ingr in cluster_ingrs[k]:\n","            idx = vocab_ingrs.add_word(ingr, idx)\n","        idx += 1\n","    _ = vocab_ingrs.add_word('<pad>', idx)\n","    for k,v in vocab_ingrs.idx2word.items():\n","        if type(v) == list:\n","            v.sort(key = lambda x: len(x))\n","\n","    print(\"Total ingr vocabulary size: {}\".format(len(vocab_ingrs)))\n","    print(\"Total token vocabulary size: {}\".format(len(vocab_toks)))\n","    dataset = {'train': [], 'val': [], 'test': []}\n","\n","    ######\n","    # 2. Tokenize and build dataset based on vocabularies.\n","    ######\n","    for i, entry in tqdm(enumerate(layer1)):\n","\n","        # get all instructions for this recipe\n","        instrs = entry['instructions']\n","\n","        instrs_list = []\n","        ingrs_list = []\n","\n","        # retrieve pre-detected ingredients for this entry\n","        det_ingrs = dets[idx2ind[entry['id']]]['ingredients']\n","        valid = dets[idx2ind[entry['id']]]['valid']\n","        labels = []\n","\n","        for j, det_ingr in enumerate(det_ingrs):\n","            if len(det_ingr) > 0 and valid[j]:\n","                det_ingr_undrs = get_ingredient(det_ingr)\n","                if len(det_ingr_undrs)>0:\n","                    if det_ingr_undrs in vocab_ingrs.word2idx:\n","                        ingrs_list.append(vocab_ingrs.idx2word[vocab_ingrs.word2idx[det_ingr_undrs]][0])\n","                    else:\n","                        ingrs_list.append(det_ingr_undrs)\n","                label_idx = vocab_ingrs(det_ingr_undrs)\n","                if label_idx is not vocab_ingrs('<pad>') and label_idx not in labels:\n","                    labels.append(label_idx)\n","\n","        # get raw text for instructions of this entry\n","        acc_len = 0\n","        for instr in instrs:\n","            instr = instr['text']\n","            instr = get_instruction(instr, replace_dict_instrs)\n","            if len(instr) > 0:\n","                acc_len += len(instr)\n","                instrs_list.append(instr)\n","\n","        # we discard recipes with too many or too few ingredients or instruction words\n","        if len(labels) < 2 or len(instrs_list) < 2 \\\n","                or len(instrs_list) >= 20 or len(labels) >= 20 \\\n","                or acc_len < 20:\n","            continue\n","\n","        # tokenize sentences\n","        toks = []\n","\n","        for instr in instrs_list:\n","            tokens = nltk.tokenize.word_tokenize(instr)\n","            toks.append(tokens)\n","\n","        title = get_instruction(entry['title'])\n","\n","        newentry = {'id': entry['id'], 'instructions': instrs_list, 'tokenized': toks,\n","                    'ingredients': ingrs_list, 'title': title}\n","        dataset[entry['partition']].append(newentry)\n","\n","    print('Dataset size:')\n","    for split in dataset.keys():\n","        print(split, ':', len(dataset[split]))\n","\n","    return vocab_ingrs, vocab_toks, dataset\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oA8xR9BUayTO","colab_type":"code","outputId":"38bb1199-2663-42ff-a258-7bca01f755ed","executionInfo":{"status":"ok","timestamp":1575159595332,"user_tz":300,"elapsed":1758858,"user":{"displayName":"Jingxian Bao","photoUrl":"","userId":"12028084690577949788"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["vocab_ingrs, vocab_toks, dataset = build_vocab_recipe1m()\n","with open(os.path.join(DATA_PATH, 'recipe1m_vocab_ingrs.pkl'), 'wb') as f:\n","  pickle.dump(vocab_ingrs, f)\n","with open(os.path.join(DATA_PATH+'recipe1m_vocab_toks.pkl'), 'wb') as f:\n","  pickle.dump(vocab_toks, f)\n","for split in dataset.keys():\n","  with open(os.path.join(DATA_PATH+'recipe1m_' + split + '.pkl'), 'wb') as f:\n","    pickle.dump(dataset[split], f)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Loading data...\n","Loaded data.\n","Found 1029720 recipes in the dataset.\n","loading pre-extracted word counters\n"],"name":"stdout"},{"output_type":"stream","text":["69it [00:00, 673.94it/s]"],"name":"stderr"},{"output_type":"stream","text":["Total ingr vocabulary size: 1909\n","Total token vocabulary size: 21487\n"],"name":"stdout"},{"output_type":"stream","text":["1029720it [26:06, 657.48it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Dataset size:\n","train : 645108\n","val : 138744\n","test : 138099\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EN75UmX2JB1q","colab_type":"code","outputId":"62307bf2-4fda-4ada-8a18-a2b55533b9f8","executionInfo":{"status":"ok","timestamp":1575162807898,"user_tz":300,"elapsed":630,"user":{"displayName":"Jingxian Bao","photoUrl":"","userId":"12028084690577949788"}},"colab":{"base_uri":"https://localhost:8080/","height":88}},"source":["print (vocab_ingrs.idx2word[vocab_ingrs.word2idx[\"mustard_powder\"]])\n","print (vocab_ingrs.idx2word[vocab_ingrs.word2idx[\"cheese\"]])\n","print (vocab_ingrs.idx2word[vocab_ingrs.word2idx[\"chicken\"]])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["['mustard', 'dry_mustard', 'hot_mustard', 'pub_mustard', 'dark_mustard', 'dijon_mustard', 'honey_mustard', 'dried_mustard', 'spicy_mustard', 'brown_mustard', 'sweet_mustard', 'cajun_mustard', 'meaux_mustard', 'maple_mustard', 'mustard_paste', 'salad_mustard', 'stout_mustard', 'vegan_mustard', 'mustard_powder', 'yellow_mustard', 'grainy_mustard', 'ground_mustard', 'creole_mustard', 'garlic_mustard', 'french_mustard', 'german_mustard', 'wasabi_mustard', 'english_mustard', 'dry_hot_mustard', 'chinese_mustard', 'pommery_mustard', 'hot_dry_mustard', 'swedish_mustard', 'prepared_mustard', 'honeycup_mustard', 'chipotle_mustard', 'sweet_hot_mustard', 'cranberry_mustard', 'dijonnaise_mustard', 'sweet_dark_mustard', 'head_honey_mustard', 'honey_dijon_mustard', 'dijon_style_mustard', 'spicy_brown_mustard', 'whole_grain_mustard', 'horseradish_mustard', 'hot_english_mustard', 'grey_poupon_mustard', 'chinese_hot_mustard', 'coarse_grain_mustard', 'prepared_hot_mustard', 'stone_ground_mustard', 'french_cream_mustard', 'french_dijon_mustard', 'french_mixed_mustard', 'dark_prepared_mustard', 'coarse_ground_mustard', 'sweet_russian_mustard', 'english_mustard_powder', 'prepared_brown_mustard', 'tarragon_dijon_mustard', 'prepared_yellow_mustard', 'mustard_mayonnaise_blend', 'grey_poupon_dijon_mustard', 'whole_grain_dijon_mustard', 'dry_english_style_mustard', 'grey_poupon_honey_mustard', 'dijon_horseradish_mustard', 'gluten_free_dijon_mustard', 'stone_ground_dijon_mustard', 'whole_grain_french_mustard', 'country_style_dijon_mustard']\n","['cheese', 'soy_cheese', 'cheese_nip', 'pot_cheese', 'oka_cheese', 'goat_cheese', 'hard_cheese', 'brie_cheese', 'feta_cheese', 'blue_cheese', 'mild_cheese', 'edam_cheese', 'soft_cheese', 'taco_cheese', 'cheese_whiz', 'milk_cheese', 'rice_cheese', 'curd_cheese', 'amul_cheese', 'bleu_cheese', 'hoop_cheese', 'cream_cheese', 'swiss_cheese', 'cheese_slice', 'colby_cheese', 'cheese_wheel', 'tasty_cheese', 'gouda_cheese', 'pizza_cheese', 'nacho_cheese', 'comte_cheese', 'mixed_cheese', 'anejo_cheese', 'light_cheese', 'white_cheese', 'piave_cheese', 'cheese_pizza', 'vegan_cheese', 'brick_cheese', 'eidam_cheese', 'sharp_cheese', 'bitto_cheese', 'kefir_cheese', 'asiago_cheese', 'chevre_cheese', 'cotija_cheese', 'romano_cheese', 'cantal_cheese', 'farmer_cheese', 'smoked_cheese', 'cheese_spread', 'teleme_cheese', 'yellow_cheese', 'yogurt_cheese', 'panela_cheese', 'oaxaca_cheese', 'paneer_cheese', 'cheese_pastry', 'burata_cheese', 'cheese_danish', 'french_cheese', 'roncal_cheese', 'syrian_cheese', 'cheddar_cheese', 'ricotta_cheese', 'cottage_cheese', 'fontina_cheese', 'low_fat_cheese', 'gruyere_cheese', 'kasseri_cheese', 'boursin_cheese', 'havarti_cheese', 'mexican_cheese', 'stilton_cheese', 'tex_mex_cheese', 'asadero_cheese', 'pimento_cheese', 'robusto_cheese', 'burrata_cheese', 'manouri_cheese', 'italian_cheese', 'robiola_cheese', 'non_fat_cheese', 'rutland_cheese', 'caprino_cheese', 'gjetost_cheese', 'morbier_cheese', 'munchee_cheese', 'rondele_cheese', 'bryndza_cheese', 'american_cheese', 'parmesan_cheese', 'velveeta_cheese', 'imperial_cheese', 'longhorn_cheese', 'manchego_cheese', 'fat_free_cheese', 'halloumi_cheese', 'pecorino_cheese', 'mizithra_cheese', 'raclette_cheese', 'muenster_cheese', 'cheshire_cheese', 'cabrales_cheese', 'montasio_cheese', 'beaufort_cheese', 'scamorza_cheese', 'caciotta_cheese', 'requeson_cheese', 'tilsiter_cheese', 'provolone_cheese', 'processed_cheese', 'roquefort_cheese', 'camembert_cheese', 'jarlsberg_cheese', 'cheese_sauce_mix', 'locatelli_cheese', 'cambozola_cheese', 'hard_goat_cheese', 'vegan_soy_cheese', 'crotonese_cheese', 'chihuahua_cheese', 'nacho_cheese_dip', 'boucheron_cheese', 'saga_blue_cheese', 'soy_cream_cheese', 'bel_paese_cheese', 'white_cheese_dip', 'anthotyro_cheese', 'limburger_cheese', 'cheese_pizza_mix', 'reblochon_cheese', 'port_wine_cheese', 'mimolette_cheese', 'mozzarella_cheese', 'neufchatel_cheese', 'mascarpone_cheese', 'fresh_goat_cheese', 'greek_feta_cheese', 'taco_blend_cheese', 'soft_cream_cheese', 'baby_swiss_cheese', 'fontinella_cheese', 'leerdammer_cheese', 'caerphilly_cheese', 'vegan_edam_cheese', 'lancashire_cheese', 'stracchino_cheese', 'light_goat_cheese', 'cheese_ravioletti', 'butterkase_cheese', 'light_cheese_whiz', 'sage_derby_cheese', 'old_cheddar_cheese', 'wensleydale_cheese', 'light_cream_cheese', 'emmenthaler_cheese', 'maytag_blue_cheese', 'danish_blue_cheese', 'fat_cottage_cheese', 'bleu_cheese_spread', 'vegan_cream_cheese', 'stravecchio_cheese', 'old_english_cheese', 'fat_ricotta_cheese', 'appenzeller_cheese', 'herbed_goat_cheese', 'reduced_fat_cheese', 'soy_cheddar_cheese', 'herbed_feta_cheese', 'vegan_swiss_cheese', 'cheddar_cheese_dip', 'montegrappa_cheese', 'aged_cheddar_cheese', 'mild_cheddar_cheese', 'smoked_gouda_cheese', 'dry_parmesan_cheese', 'lowfat_swiss_cheese', 'laughing_cow_cheese', 'smoked_swiss_cheese', 'low_fat_goat_cheese', 'parmesan_soy_cheese', 'cheddar_cheese_cube', 'low_fat_soft_cheese', 'cacio_di_rom_cheese', 'cooper_sharp_cheese', 'veggie_shred_cheese', 'frozen_cheese_pizza', 'saint_paulin_cheese', 'asiago_cheese_blend', 'mild_mexican_cheese', 'whipped_cream_cheese', 'sharp_cheddar_cheese', 'low_fat_cream_cheese', 'monterey_jack_cheese', 'italian_cheese_blend', 'white_cheddar_cheese', 'light_cheddar_cheese', 'blue_cheese_dressing', 'jalapeno_jack_cheese', 'mexican_cheese_blend', 'light_ricotta_cheese', 'mexican_blend_cheese', 'low_fat_swiss_cheese', 'port_du_salut_cheese', 'light_cottage_cheese', 'vegan_cheddar_cheese', 'fat_free_feta_cheese', 'light_boursin_cheese', 'low_fat_quark_cheese', 'red_leicester_cheese', 'cheese_cracker_crumb', 'bacon_cheddar_cheese', 'cheese_stuffed_shell', 'cheese_and_salsa_dip', 'fat_free_cream_cheese', 'fresh_parmesan_cheese', 'nonfat_cheddar_cheese', 'nonfat_cottage_cheese', 'smoked_cheddar_cheese', 'sharp_american_cheese', 'velveeta_cheese_slice', 'cheddar_cheese_spread', 'cracker_barrel_cheese', 'vegan_parmesan_cheese', 'mature_cheddar_cheese', 'processed_cheese_food', 'kraft_parmesan_cheese', 'soy_mozzarella_cheese', 'kefalograviera_cheese', 'fat_free_swiss_cheese', 'provolone_cheese_cube', 'semi_soft_goat_cheese', 'roquefort_blue_cheese', 'sharp_longhorn_cheese', 'smoked_gruyere_cheese', 'danish_fontina_cheese', 'cheddar_cheese_powder', 'nonfat_ricotta_cheese', 'bacon_flavored_cheese', 'herbed_cheddar_cheese', 'vegan_american_cheese', 'low_fat_cottage_cheese', 'low_fat_cheddar_cheese', 'pecorino_romano_cheese', 'low_fat_ricotta_cheese', 'soft_fresh_goat_cheese', 'white_processed_cheese', 'nonfat_parmesan_cheese', 'american_cheese_spread', 'jalapeno_pepper_cheese', 'honey_nut_cream_cheese', 'light_processed_cheese', 'processed_swiss_cheese', 'nice_and_cheesy_cheese', 'lowfat_parmesan_cheese', 'low_fat_mexican_cheese', 'vegetarian_blue_cheese', 'low_cholesterol_cheese', 'irish_farmhouse_cheese', 'mozzarella_cheese_cube', 'mull_of_kintyre_cheese', 'light_camembert_cheese', 'asiago_parmesan_cheese', 'light_jarlsberg_cheese', 'habanero_pepper_cheese', 'norwegian_brown_cheese', 'shredded_cheddar_cheese', 'process_american_cheese', 'fat_free_cheddar_cheese', 'fat_free_cottage_cheese', 'fresh_mozzarella_cheese', 'velveeta_mexican_cheese', 'fat_free_ricotta_cheese', 'reduced_fat_feta_cheese', 'processed_cheese_spread', 'reduced_fat_blue_cheese', 'vegan_mozzarella_cheese', 'low_fat_american_cheese', 'low_fat_parmesan_cheese', 'light_mozzarella_cheese', 'dry_curd_cottage_cheese', 'strawberry_cream_cheese', 'jalapeno_havarti_cheese', 'smoked_provolone_cheese', 'jalapeno_cheddar_cheese', 'reduced_fat_goat_cheese', 'chipotle_cheddar_cheese', 'canadian_cheddar_cheese', 'vegetarian_cream_cheese', 'cheese_flavored_hot_dog', 'cheese_flavored_granule', 'part_skim_farmer_cheese', 'mozzarella_string_cheese', 'part_skim_ricotta_cheese', 'smoked_mozzarella_cheese', 'reduced_fat_cream_cheese', 'lowfat_mozzarella_cheese', 'reduced_fat_swiss_cheese', 'fat_free_parmesan_cheese', 'fat_free_american_cheese', 'skim_milk_ricotta_cheese', 'reduced_fat_tasty_cheese', 'double_cream_brie_cheese', 'non_dairy_cheddar_cheese', 'cheddar_and_colby_cheese', 'lowfat_neufchatel_cheese', 'double_gloucester_cheese', 'reduced_fat_colby_cheese', 'mexican_processed_cheese', 'frozen_four_cheese_pizza', 'part_skim_cottage_cheese', 'low_fat_provolone_cheese', 'part_skim_cheddar_cheese', 'chefstyle_cheddar_cheese', 'philadelphia_cream_cheese', 'whole_milk_ricotta_cheese', 'small_curd_cottage_cheese', 'kraft_macaroni_and_cheese', 'macaroni_shell_and_cheese', 'large_curd_cottage_cheese', 'light_cream_cheese_spread', 'aged_white_cheddar_cheese', 'vegetarian_cheddar_cheese', 'reduced_fat_romano_cheese', 'low_sodium_cheddar_cheese', 'pecorino_siciliano_cheese', 'low_fat_mascarpone_cheese', 'lactose_free_cream_cheese', 'whole_milk_cottage_cheese', 'vegetarian_stilton_cheese', 'vegetarian_gruyere_cheese', 'parmigiano_reggiano_cheese', 'reduced_fat_cheddar_cheese', 'extra_sharp_cheddar_cheese', 'colby_monterey_jack_cheese', 'fat_free_mozzarella_cheese', 'sharp_white_cheddar_cheese', 'cream_style_cottage_cheese', 'reduced_fat_ricotta_cheese', 'horseradish_cheddar_cheese', 'low_sodium_parmesan_cheese', 'vegetarian_parmesan_cheese', 'simply_macaroni_and_cheese', 'reduced_fat_havarti_cheese', 'light_monterey_jack_cheese', 'vegan_monterey_jack_cheese', 'smart_beat_fat_free_cheese', 'frozen_macaroni_and_cheese', 'english_stilchester_cheese', 'whipped_onion_cream_cheese', 'monterey_jack_pepper_cheese', 'part_skim_mozzarella_cheese', 'black_pepper_boursin_cheese', 'reduced_fat_parmesan_cheese', 'reduced_fat_american_cheese', 'onion_and_herb_cream_cheese', 'extra_mature_cheddar_cheese', 'velveeta_pepper_jack_cheese', 'lincolnshire_poacher_cheese', 'fruit_flavored_cream_cheese', 'roasted_garlic_cream_cheese', 'cheese_and_jalapeno_dip_mix', 'reduced_fat_dubliner_cheese', 'garlic_and_herb_goat_cheese', 'smoked_swiss_cheddar_cheese', 'cheese_herb_filled_pansotti', 'kraft_grated_parmesan_cheese', 'prepared_macaroni_and_cheese', 'low_fat_monterey_jack_cheese', 'chive_and_onion_cream_cheese', 'low_fat_sharp_cheddar_cheese', 'reduced_fat_jarlsberg_cheese', 'vermont_sharp_cheddar_cheese', 'low_fat_white_cheddar_cheese', 'gorgonzola_dolce_blue_cheese', 'taco_seasoned_cheddar_cheese', 'reduced_calorie_cream_cheese', 'kraft_processed_cheese_slice', 'reduced_fat_provolone_cheese', 'low_sodium_mozzarella_cheese', 'olive_and_onion_cream_cheese', 'low_fat_whipped_cream_cheese', 'cranberry_walnut_goat_cheese', 'kraft_shredded_cheddar_cheese', 'fat_free_monterey_jack_cheese', 'fat_free_sharp_cheddar_cheese', 'reduced_fat_mozzarella_cheese', 'sharp_processed_cheese_spread', 'mushroom_flavored_brie_cheese', 'mozzarella_flavor_rice_cheese', 'sun_dried_tomato_cream_cheese', 'reduced_fat_neufchatel_cheese', 'low_fat_vanilla_yogurt_cheese', 'sharp_canadian_cheddar_cheese', 'kraft_shredded_parmesan_cheese', 'pre_shredded_mozzarella_cheese', 'philadelphia_neufchatel_cheese', 'monterey_jack_style_soy_cheese', 'jalapeno_flavored_cream_cheese', 'laughing_cow_light_swiss_cheese', 'kraft_old_english_cheese_spread', 'mozzarella_cheddar_blend_cheese', 'reduced_fat_mild_cheddar_cheese', 'roasted_red_pepper_cream_cheese', 'low_fat_low_sodium_swiss_cheese', 'low_fat_jalapeno_cheddar_cheese', 'velveeta_american_cheese_spread', 'vidalia_onion_spreadable_cheese', 'lowfat_large_curd_cottage_cheese', 'reduced_fat_sharp_cheddar_cheese', 'reduced_fat_monterey_jack_cheese', 'four_cheese_mexican_blend_cheese', 'kraft_shredded_mozzarella_cheese', 'tofutti_better_than_cream_cheese', 'head_all_natural_muenster_cheese', 'low_fat_small_curd_cottage_cheese', 'garlic_and_herb_spreadable_cheese', 'cheddar_and_american_blend_cheese', 'non_fat_small_curd_cottage_cheese', 'cheese_zesty_mexican_cheese_blend', 'low_fat_low_sodium_cottage_cheese', 'kraft_shredded_mild_cheddar_cheese', 'kraft_milk_shredded_cheddar_cheese', 'light_chive_and_onion_cream_cheese', 'alfredo_sauce_with_parmesan_cheese', 'low_fat_smoked_salmon_cream_cheese', 'part_skim_mozzarella_string_cheese', 'reduced_fat_raspberry_cream_cheese', 'cracker_barrel_sharp_cheddar_cheese', 'kraft_shredded_sharp_cheddar_cheese', 'kraft_shredded_monterey_jack_cheese', 'shredded_reduced_fat_cheddar_cheese', 'low_fat_monterey_jack_pepper_cheese', 'cheese_gourmet_cheddar_blend_cheese', 'crystal_farm_shredded_cheddar_cheese', 'alpine_lace_reduced_fat_swiss_cheese', 'fat_free_monterey_jack_pepper_cheese', 'kraft_milk_shredded_mozzarella_cheese', 'country_casserole_cheese_blend_cheese', 'frozen_hash_brown_with_cheddar_cheese', 'low_fat_jalapeno_monterey_jack_cheese', 'reduced_fat_colby_monterey_jack_cheese', 'low_fat_monterey_jack_and_colby_cheese', 'reduced_fat_extra_sharp_cheddar_cheese', 'cheddar_and_monterey_jack_blend_cheese', 'instant_mashed_potato_with_four_cheese', 'low_fat_low_sodium_monterey_jack_cheese', 'reduced_fat_reduced_sodium_swiss_cheese', 'powdered_cheese_mix_from_mac_and_cheese', 'polly_o_natural_part_skim_ricotta_cheese', 'low_moisture_part_skim_mozzarella_cheese', 'reduced_fat_reduced_sodium_cheese_spread', 'laughing_cow_light_garlic_and_herb_cheese', 'alpine_lace_reduced_fat_hot_pepper_cheese', 'light_garden_vegetable_cream_cheese_spread', 'sour_cream_and_chive_flavored_cream_cheese', 'alouette_spinach_artichoke_spreadable_cheese', 'kraft_mexican_style_finely_shredded_four_cheese', 'philadelphia_chive_and_onion_cream_cheese_spread', 'alouette_sundried_tomato_and_basil_spreadable_cheese', 'philadelphia_chive_and_onion_le_fat_than_cream_cheese', 'kraft_shredded_low_moisture_part_skim_mozzarella_cheese']\n","['chicken', 'raw_chicken', 'chicken_wing', 'chicken_part', 'chicken_base', 'lean_chicken', 'chicken_skin', 'chicken_half', 'chicken_bone', 'chicken_back', 'chicken_stew', 'chicken_neck', 'deli_chicken', 'baby_chicken', 'chicken_thigh', 'fryer_chicken', 'chicken_strip', 'whole_chicken', 'vegan_chicken', 'chicken_chunk', 'chunk_chicken', 'fried_chicken', 'chicken_salad', 'chicken_heart', 'chicken_breast', 'chicken_tender', 'cooked_chicken', 'chicken_cutlet', 'ground_chicken', 'minced_chicken', 'smoked_chicken', 'chicken_giblet', 'frying_chicken', 'canned_chicken', 'chicken_a_king', 'spring_chicken', 'chicken_burger', 'chicken_chilli', 'chicken_carcass', 'stewing_chicken', 'grilled_chicken', 'broiler_chicken', 'chicken_gizzard', 'chicken_portion', 'chicken_quarter', 'popcorn_chicken', 'chicken_hot_dog', 'boneless_chicken', 'roasting_chicken', 'skinless_chicken', 'chicken_taquitos', 'stir_fry_chicken', 'chicken_drumstick', 'chicken_soup_base', 'chicken_gravy_mix', 'chicken_schnitzel', 'chicken_leg_thigh', 'barbecued_chicken', 'chicken_baby_food', 'chicken_escalopes', 'chicken_stock_cube', 'chicken_drummettes', 'free_range_chicken', 'white_meat_chicken', 'chicken_breast_half', 'chicken_soup_powder', 'lean_ground_chicken', 'chicken_coating_mix', 'chicken_leg_quarter', 'chicken_rice_a_roni', 'chicken_stock_paste', 'whole_chicken_breast', 'chicken_breast_halve', 'chicken_stock_powder', 'roasted_deli_chicken', 'ground_chicken_thigh', 'chicken_curry_powder', 'chicken_bouillon_mix', 'chicken_seitan_strip', 'chicken_breast_tender', 'chicken_bouillon_cube', 'broiler_fryer_chicken', 'cooked_chicken_breast', 'head_chipotle_chicken', 'smoked_chicken_breast', 'ground_chicken_breast', 'premium_chunk_chicken', 'skinless_chicken_half', 'chicken_stock_granule', 'chicken_back_and_neck', 'grilled_chicken_breast', 'boneless_chicken_thigh', 'skinless_chicken_thigh', 'breaded_chicken_tender', 'roasted_chicken_breast', 'chicken_leg_with_thigh', 'chicken_rice_pilaf_mix', 'breaded_chicken_breast', 'young_roasting_chicken', 'breaded_chicken_cutlet', 'chicken_mid_joint_wing', 'boneless_chicken_breast', 'skinless_chicken_breast', 'chicken_bouillon_powder', 'chicken_noodle_soup_mix', 'chicken_stock_reduction', 'farmland_chicken_burger', 'chicken_bouillon_granule', 'vegetarian_chicken_strip', 'skinless_chicken_quarter', 'free_range_chicken_thigh', 'boneless_skinless_chicken', 'rotisserie_cooked_chicken', 'oven_roasted_deli_chicken', 'skinless_roasting_chicken', 'pan_dripping_from_chicken', 'skinless_chicken_drumstick', 'cooked_chicken_breast_half', 'southwestern_chicken_strip', 'tyson_fajita_chicken_strip', 'chicken_enchilada_soup_mix', 'tyson_fresh_ground_chicken', 'white_meat_chicken_quarter', 'boneless_chicken_drumstick', 'cooked_chicken_breast_strip', 'chicken_flavor_stuffing_mix', 'boneless_chicken_breast_half', 'skinless_chicken_breast_half', 'grilled_chicken_breast_strip', 'skinless_chicken_leg_quarter', 'chicken_bouillon_concentrate', 'whole_boneless_chicken_breast', 'chicken_quesadilla_pasta_roni', 'whole_skinless_chicken_breast', 'chicken_flavored_bullion_cube', 'chicken_and_herb_couscous_mix', 'cooked_italian_chicken_breast', 'chicken_stove_top_stuffing_mix', 'smoked_skinless_chicken_breast', 'chicken_and_wild_rice_soup_mix', 'boneless_skinless_chicken_thigh', 'boneless_skinned_chicken_breast', 'boneless_skinless_chicken_breast', 'boneless_skinless_chicken_cutlet', 'worcestershire_sauce_for_chicken', 'kraft_oven_fry_extra_crispy_chicken', 'southwestern_seasoned_chicken_breast', 'chicken_and_herb_classico_rice_a_roni', 'whole_boneless_skinless_chicken_breast', 'chicken_flavor_rice_and_vermicelli_mix', 'cooked_boneless_skinless_chicken_breast', 'chicken_and_broccoli_flavor_rice_a_roni', 'boneless_skinless_smoked_chicken_breast', 'chicken_flavor_long_grain_wild_rice_mix', 'barbecue_seasoned_boneless_chicken_breast', 'chicken_breast_marinated_in_tandoori_paste', 'morningstar_farm_meal_starter_chicken_strip', 'refrigerated_taco_sauce_with_seasoned_shredded_chicken', 'perdue_short_cut_original_roasted_carved_chicken_breast']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2VWFzP6vLHk8","colab_type":"code","colab":{}},"source":["data = pickle.load(open(DATA_PATH + 'recipe1m_train.pkl', 'rb'))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HcAdwTg76BS-","colab_type":"code","outputId":"da329a8c-229d-409e-bd55-ab559f8d5af6","executionInfo":{"status":"ok","timestamp":1575162886791,"user_tz":300,"elapsed":267,"user":{"displayName":"Jingxian Bao","photoUrl":"","userId":"12028084690577949788"}},"colab":{"base_uri":"https://localhost:8080/","height":105}},"source":["i = 0\n","print (data[i]['id'])\n","print (data[i]['title'])\n","print (data[i]['ingredients'])\n","print (data[i]['instructions'])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["000033e39b\n","dilly macaroni salad recipe\n","['macaroni', 'cheese', 'celery', 'pepper', 'pimento', 'mayonnaise', 'vinegar', 'salt', 'dill']\n","['cook macaroni according to package directions. drain well.', 'cold.', 'combine macaroni, cheese cubes, celery, green pepper and pimento.', 'blend together mayonnaise or possibly salad dressing, vinegar, salt and dill weed. add in to macaroni mix.', 'toss lightly.', 'cover and refrigeratewell.', 'serve salad in lettuce lined bowl if you like.', 'makes 6 servings.']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_F92gtDRqXWx","colab_type":"text"},"source":["# Data Sampling"]},{"cell_type":"code","metadata":{"id":"rDdROBupeGOs","colab_type":"code","outputId":"da0b3d90-9c5e-4ea9-be74-4ad1a5034f6e","executionInfo":{"status":"ok","timestamp":1575154603900,"user_tz":300,"elapsed":1840281,"user":{"displayName":"Jingxian Bao","photoUrl":"","userId":"12028084690577949788"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["# 1/n of original data set (sample)\n","#!mkdir $DATA_PATH+'sample-100'\n","for split in [\"train\",\"val\",\"test\"]:\n","    with open(os.path.join(DATA_PATH+'sample-100/recipe1m_' + split + '.pkl'), 'wb') as f:\n","        data = pickle.load(open(DATA_PATH + 'recipe1m_%s.pkl'%split, 'rb'))\n","        data = random.sample(data,int(len(data)/100))\n","        print(split+':'+str(len(data)))\n","        pickle.dump(data, f)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["train:6451\n","val:1387\n","test:1380\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qM6gKRG-WgiF","colab_type":"code","outputId":"70042d5c-ea0b-4c10-e1ae-a1a5600c62a5","executionInfo":{"status":"ok","timestamp":1575163043100,"user_tz":300,"elapsed":137758,"user":{"displayName":"Jingxian Bao","photoUrl":"","userId":"12028084690577949788"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# 1/n of original data set (slicing)\n","!mkdir \"$DATA_PATH\"\"sample-20\"\n","n = 20\n","for split in [\"train\",\"val\",\"test\"]:\n","    data = pickle.load(open(DATA_PATH + 'recipe1m_%s.pkl'%split, 'rb'))\n","    for i in range (n):\n","        with open(os.path.join(DATA_PATH+'sample-%d/recipe1m_'%n + split +'-'+ str(i)+'.pkl'), 'wb') as f:\n","            split_data = data[i*int(len(data)/n):(i+1)*int(len(data)/n)]\n","            print(split+'-'+str(i)+':'+str(len(split_data)))\n","            pickle.dump(split_data, f)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["mkdir: cannot create directory ‘/content/drive/My Drive/Intro to AI Project/data/sample-20’: File exists\n","train-0:32255\n","train-1:32255\n","train-2:32255\n","train-3:32255\n","train-4:32255\n","train-5:32255\n","train-6:32255\n","train-7:32255\n","train-8:32255\n","train-9:32255\n","train-10:32255\n","train-11:32255\n","train-12:32255\n","train-13:32255\n","train-14:32255\n","train-15:32255\n","train-16:32255\n","train-17:32255\n","train-18:32255\n","train-19:32255\n","val-0:6937\n","val-1:6937\n","val-2:6937\n","val-3:6937\n","val-4:6937\n","val-5:6937\n","val-6:6937\n","val-7:6937\n","val-8:6937\n","val-9:6937\n","val-10:6937\n","val-11:6937\n","val-12:6937\n","val-13:6937\n","val-14:6937\n","val-15:6937\n","val-16:6937\n","val-17:6937\n","val-18:6937\n","val-19:6937\n","test-0:6904\n","test-1:6904\n","test-2:6904\n","test-3:6904\n","test-4:6904\n","test-5:6904\n","test-6:6904\n","test-7:6904\n","test-8:6904\n","test-9:6904\n","test-10:6904\n","test-11:6904\n","test-12:6904\n","test-13:6904\n","test-14:6904\n","test-15:6904\n","test-16:6904\n","test-17:6904\n","test-18:6904\n","test-19:6904\n"],"name":"stdout"}]}]}